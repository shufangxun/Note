# PCA与SVD关系 [参考](cnblogs.com/pinard/p/6239403.html)

## PCA  

PCA属于**无监督降维**，找出数据里最主要成分去代替原始数据。具体的，设数据集是n维的，共有$m$个数据$(x(1),x(2),...,x(m))$。希望将这m个数据的维度从$n$维降到$k$维，并且新数据集尽可能的代表原始数据集。

**方法**：基于最大投影方差或者最小投影距离

## 流程

- 在**特征维度**数据归一化
- 计算样本协方差矩阵 $XX^{T}$
- 求协方差矩阵特征值，取出前n个最大的特征向量并标准化，组成特征矩阵W
- 将原数据每一个样本通过 W 变换为新样本 $mn*nk = mk$

## 优点

- 仅以方差衡量信息量，不受其他因素影响
- 各主成分之间正交，消除原始数据成分间的相互影响的因素
- 主要运算是特征值分解，易于实现

## 缺点

- 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。
- 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

## PCA与SVD的关系和区别

- SVD维度变换是 $mn = mm * mn * nn$，可以**两个方向上主成分**，而PCA只能获得单个方向上的主成分，而PCA仅用SVD的右奇异矩阵
- SVD左奇异矩阵可以用于行数的压缩，右奇异矩阵可以用于列数压缩
- SVD有一些实现算法可以不用先求出协方差矩阵，也能求出右奇异矩阵。也就是说，PCA可以不用做特征分解，而是做SVD来完成。这在样本量很大的时候很有效。

## PCA可以用来降低过拟合么  

不适合，因为PCA是无监督降维，**没有考虑标签**，而且方差小的特征可能对数据有很大作用
