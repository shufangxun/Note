# 决策树

##  1. 基础树
### ID3
ID3 使用的分裂标准是信息增益，它表示已知特征A使得样本集合不确定性减少的程度
$$
G(D,A) = H(D) - H(D|A)
$$
信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”
#### 算法步骤
1. 初始化；
2. 计算信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；
3. 更新数据和特征（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；
4. 重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。
#### 缺点
- ID3 没有剪枝策略，容易过拟合；
- 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；
- 只能用于处理离散分布的特征, 没有考虑缺失值

### C4.5
在$ID3$的基础上做了一些改进，最核心的改进是把信息增益换成了信息增益比，克服了$ID3$对特征数目的偏重缺点
$$
G_{ratio}(D,A) = \frac{G(D,A)}{H_A(D)}
$$
注意，信息增益比对可取值较少的特征有偏好（**分母越小，整体越大**），因此$C4.5$使用一个启发式方法：先从候选特征中找到信息增益高于平均值的特征，再从中选择增益比最高的

#### 缺点
- 剪枝策略可以再优化；
- C4.5 用的是多叉树，用二叉树效率更高；
- C4.5 只能用于分类；
C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；


### CART
$CART$是二叉树，采用基尼指数作为二分标准，计算便捷，基尼系数越小，不纯度越低，特征越好，这和信息增益（率）正好相反。

$$
Gini(D)= \sum \frac{|C_k|}{|D|}(1- \frac{|C_k|}{|D|})
$$

基尼指数反映了从**数据集中随机抽取两个样本，其类别标记不一致的概率**，可以理解为熵模型的一阶泰勒展开

#### 剪枝策略
见参考链接


#### 回归树
CART（Classification and Regression Tree，分类回归树），从名字就可以看出其不仅可以用于分类，也可以应用于回归

回归树使用误差平方和最小准则，对于任意特征A，对应的任意划分点s两边划分成的数据集$D_1$和$D_2$ ，求出使各自均方差最小，同时均方差之和最小所对应的特征和特征值划分点：
$$
\min_{a,s}[\min_{c_1} \sum_{D_1}(y-c_1)^2 + \min_{c_2} \sum_{D_2}(y-c_2)^2]
$$
回归树输出不是类别，而是最终叶子的均值或者中位数

## 2. 集成学习
决策树的优点是简单，逻辑清晰，具备可解释性，但是也有一个很大的缺点：非常容易过拟合，解决过拟合的方法主要是有剪枝、集成学习，本章主要讲集成学习，集成学习就是**多个基分类器（一般是弱分类器，比如决策树）组合成一个强分类器**，其泛化能力自然会比单个弱分类器效果更好。

### Bagging
Bagging是一种有放回抽样方法：取出一个样本加入训练集，然后再把该样本放回样本空间，这样可以得到多个子训练集。然后**每个基分类器基于不同子训练集进行训练**，并综合所有基分类器的预测值得到最终的预测结果，基分类器之间的相关性较弱。

### Boosting 
Boosting是迭代训练，**每一个弱分类器基于上一个弱分类器的结果加权训练**，常见方法如：提高错误样本权重，降低正确样本权重；提高误差小的弱分类器权重，基分类器之间相关性较强。

### 偏差方差角度理解

偏差刻画的是算法本身的拟合能力，也即模型的准确性；方差度量了数据扰动所带来的影响，也即模型的稳定性。为了泛化能力尽可能的好，就需要让偏差和方差都减小，但这两者一般来说是相对矛盾的。

Bagging和Boosting，前者集成多个独立弱分类器提高泛化能力，以**降低方差**为目标；后者采用重赋权的迭代算法，每次迭代根据上次预测结果进行加权，以**降低偏差**为目标。

在 Bagging 和 Boosting 中，通过计算基模型期望和方差可以得到模型整体期望和方差。假设基模型的**期望为$\mu$**，**方差$\sigma^2$**，**模型的权重为$r$** ，两两模型间的**相关系数$\rho$**，那么：
**模型偏差**：
$$
\begin{aligned}
E(F) & = E(\sum_{i}^{m}r_if_i) \\
  & = \sum_{i}^{m}r_iE(f_i)
\end{aligned}
$$

**模型方差**：
$$
\begin{aligned}
Var(F) & = Var(\sum_{i}^{m}r_if_i) \\
  & = \sum_{i}^{m}Var(r_if_i) +  \sum_{i \neq j}^{m}Conv(r_if_i, r_jf_j)\\
  & = \sum_{i}^{m}r_i^2Var(f_i) + \sum_{i \neq j}^{m}\rho r_ir_j \sqrt{Var(f_i)}\sqrt{Var(f_j)} \\
  & = mr^2\sigma^2 + \rho r^2 \sigma^2 m(m-1) \\
  & = mr^2\sigma^2(1-\rho) + m^2\rho r^2 \sigma^2
\end{aligned}
$$

模型的准确度可由偏差和方差共同决定:
$$
Error = bias^2 + var + \epsilon
$$

#### Bagging的偏差和方差
对于 Bagging 来说，每个基模型的权重$r$等于$\frac{1}{m}$且期望$\mu$近似相等，故我们可以得到：
$$
\begin{aligned}
& E(F) = \mu \\
& Var(F) = \frac{1}{m} \sigma^2 (1-\rho) + \rho \sigma^2
\end{aligned}
$$


可以得到：
- 集成模型的偏差和基模型一样
- 集成模型的方差小于基模型，且基模型越多，第一项很小，但有上限；同时由于基模型相关性很小，所以第二项很小，因此方差小

#### Boosting的偏差与方差
Boosting 由于基模型共用同一套训练集，所以基模型间具有强相关性，故模型间的相关系数近似等于 1：
$$
\begin{aligned}
& E(F) = \sum_{i}^{m}r_iE(f_i) \\
& Var(F)=\sigma^2
\end{aligned}
$$
可以得到：
- 集成模型的方差等于基模型
- 集成模型的偏差可以通过迭代训练减小
- 注意其实boosting方法也可以减少方差，类似于随机森林的处理，对特征进行随机抽样来使基模型间的**相关性降低**
  
因此 Bagging 中的基模型为强模型(**偏差低，方差高**)，而Boosting 中的基模型为弱模型(**偏差高，方差低**）

### Random Forest
随机森林是 Bagging 方法的扩展，由多个决策树组成，每一棵决策树之间没有相关性，在此基础上引入随机特征选择：先随机选择一个包含k个特征的子集，然后再从子集中选择一个最优特征进行划分，这也是随机的内涵。

#### 算法
1. 随机选择样本；
2. 随机选择特征；
3. 构建决策树；
4. 随机森林投票（平均）

#### 优点
- 采样阶段的随机样本
- 构建树阶段引入的随机特征

### Adaboost
AdaBoost（Adaptive Boosting，自适应增强），其自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。

#### 算法
1. 初始化训练样本的权值分布，每个样本具有相同权重；
2. 训练弱分类器，**如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高**。用更新过的样本集去训练下一个分类器；
3. 将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，**加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重。**

#### 优点
- 分类精度高；
- 可以用各种回归分类模型来构建弱学习器，非常灵活；
- 不容易发生过拟合。

#### 缺点
- 对异常点敏感，异常点会获得较高权重
  
### GBDT
GBDT（Gradient Boosting Decision Tree）由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB）和 Shringkage（一个重要演变）

总结来说，GBDT算法基树采用CART回归树，树节点的划分指标是平方损失函数，叶子节点的值是落在该叶子节点所有样本的目标均值。树与树之间的Boosting逻辑是：新树拟合的目标是上一课树的损失函数的负梯度。GBDT最终的输出结果是将样本在所有树上的叶子值相加。

GBDT最核心的两部分，Boosting提升说明每棵树之间是有关系有序的、Gradient梯度指明了提升的方向与大小

#### 提升决策树
随机森林内各个树之间是没有关联的，提升树针对这一点做了该静：**每棵树都是以前一棵树的残差为学习目标去拟合**，模型最终的输出是将所有树的结果相加。这也是说为什么GBDT的树全都是**回归树**的原因——因为**分类树的结果不能使用加法模型，也就是无法提升**

#### 负梯度提升
提升决策树时，默认每棵树的提升是以上一棵树的残差为拟合目标残差，这是因为回归树的损失函数是平方损失函数，但是当损失函数不是平方损失函数时，用损失函数的负梯度去优化。可以这么说，提升决策树中的拟合残差是拟合负梯度的一种特例。

一般回归类的损失函数会用绝对损失或者 Huber 损失函数来代替平方损失函数，**GBDT的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于0**，这样后面的树就能专注于那些被分错的样本。

#### 缩减

Shrinkage的思想认为，每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。

Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大。

#### 与Adaboost区别
1. 迭代思路不同：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）；
2. 损失函数不同：AdaBoost 采用的是指数损失，GBDT 使用的是绝对损失或者 Huber 损失函数；


## 3. 工业实践

在GBDT之后，其提出的xgboost和lightgbm都只是高效的系统实现，并不算是一种崭新算法。因此，主要从与已有算法的差异来讲一下两者，最后再补充讲一下用于选取模型参数的贝叶斯优化

### Xgboost





## 4. 问答

**1. 决策树的两个关键问题？**
- 如何选择较优的特征属性进行分裂？每一次特征属性的分裂，相当于对数据集进行再划分，也对应了一次决策树的生长。所以说，需要定义一个目标函数
- 什么时候该停止分裂？有两种自然情况需要停止分裂，一是该节点对应的所有样本记录均属于同一类别，二是该节点对应的所有样本特征属性值均相等。但是否还有其他情况呢？


**2. 介绍一下决策树**

基于特征对数据进行分类的模型，每次选取**单一特征**进行，可以看作是if-then规则的集合，主要流程分为三个：特征选择，决策树生成，决策树剪枝

**3. 决策树怎么做的？**  

- **特征选择 ** 

  ID3: 以信息增益最大化为标准，$G(D,A) = H(D) - H(D|A)$
  
  C4.5: 以信息增益比最大化为标准
  
  CART：分类树以基尼系数最小为标准，回归树以误差平方和最小为标准
  
- **决策树生成**

  将所有数据放在根节点，根据评价指标选取特征，直到没有特征可选

- **决策树剪枝**

  决策树倾向于正确分类当前数据，泛化能力因此会变弱，剪枝就是为了减轻**过拟合问题**，自底向上处理叶子结点

**4. 为什么这么操作？**

- 为什么要信息增益最大

  $H(D|A)$ 是以特征$A$分类时样本的不确定性，为了更好地分类，希望选取$A$特征之后样本的不确定性能够尽可能小，所以采用信息增益最大化

- 为什么信息增益为指标倾向于取值较多地特征

  原因同上

**5. 决策树对样本不平衡的敏感性？**
不敏感，因为决策树使用基于类变量的划分规则去创建因此可以强制地将不同类别的样本分开。

样本不平衡的思考角度：
> 1. 数据层面（过采样、欠采样）；
> 2. 算法调参层面（代价敏感学习，LR算法class_weight参数设置）；
> 3. 尝试使用对样本类别不平衡不敏感的算法（决策树、SVM）3个层面考虑。

**6. 为什么说bagging降低了方差，boosting降低了偏差？**
- [解答](https://www.zhihu.com/question/26760839)



#### 参考链接
[1. 基础树](https://zhuanlan.zhihu.com/p/85731206)
[2. Random Forest、Adaboost、GBDT](https://zhuanlan.zhihu.com/p/86263786)