# L1 和 L2 正则化

Loss后面加上**权重的正则项**，平衡经验风险最小化和结构风险最小化，尽可能采用简单的模型，降低模型复杂度，以此提高模型泛化能力

## 理解

- 正则化产生了稀疏性（Sparsity），减少了特征向量个数，自动选择主要特征，降低了模型的复杂度，提高了泛化能力，符合**奥卡姆剃刀原理**
- 贝叶斯角度：正则化项的引入了**先验知识**约束模型，L1 属于拉普拉斯分布先验，L2 属于高斯分布先验

**L1正则化**  
在Loss后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）

$$
f_{t}^{reg}(w) = f_{t}(w) + \frac{\lambda ^{reg}}{2}||w||_{1}
$$

**L2正则化**  
在Loss后边所加正则项为L2范数，加上L2范数得到的解比较平滑，大部分都很小

$$
f_{t}^{reg}(w) = f_{t}(w) + \frac{\lambda ^{reg}}{2}||w||_{2}^{2}
$$

**L1和L2正则化使用范围**  
只用于卷积层和全连接层，因为这里参数很多，如果用于BN层，参数过少，会丧失功能

## L2正则化和weight decay的区别

### weight decay

权重衰减在训练的每一步结束的时候，对网络中的参数值**裁剪一定的比例**，优化目标是不变的

$$
w_{t+1} = (1- \lambda)w_{t} - \alpha \frac{\partial f_{t}(w)}{\partial w_{t}}
$$

### L2 正则化

在Loss中直接加上一个正则项，直接修改了优化目标，权重更新变为：

$$
w_{t+1} = w_{t} - \alpha (\frac{\partial f_{t}(w)}{\partial w_{t}} + \lambda ^{reg}w_{t})
$$

### 关系

在**标准梯度下降**中，当 $\alpha \lambda_{reg} = \lambda$ 时，L2正则化等价于权重衰减
