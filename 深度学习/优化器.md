# 优化器总结

## 符号和框架

学习率 $α$、动量参数 $β$、动量 $v$、权重 $w$、目标函数：$f(w)$

在epoch $t$：

1. 计算目标函数关于当前参数的梯度：
   $$g_{t} =\bigtriangledown  f(w_{t})$$
2. 计算一阶动量和二阶动量：  
   $$
   \begin{aligned}
   v_{t} = \phi(g_{1},g_{2},...)  \\
   s_{t} = \psi(g_{1},g_{2},...)
   \end{aligned}
    $$
3. 计算当前时刻的下降梯度：
   $$\eta_{t} =  \alpha\frac{v_{t}}{\sqrt{s_{t}}}$$
4. 根据下降梯度进行更新：  
   $$w_{t+1} = w_{t} - \eta_{t}$$

## SGD

SGD没有动量的概念，也就是：
$$
   \begin{aligned}
   &\ \ v_{t}=g_{t} \\
   &\ \ s_{t}=I
   \end{aligned}
$$
则梯度下降更新为:  
$$w_{t+1} = w_{t} - \alpha g_{t}$$

缺点

- 学习率统一变化 - SGD对**所有参数更新使用同样学习率**。但对于稀疏数据或者特征，想更新快一些；对于常出现的特征更新慢一些

- 下降速度慢，并且可能会在沟壑的两边持续震荡，停留在一个局部最优点

## SGD with Momentum

为了抑制振荡，在一阶动量中加入惯性：
$$
   \begin{aligned}
  &\ \ v_{t}=\beta v_{t-1} + (1-\beta)g_{t} \\
  &\ \ s_{t}=I
   \end{aligned}
$$

则梯度下降更新为:  
$$
w_{t+1} = w_{t} - \alpha v_{t}
$$

优点

- 每一次的参数更新方向不仅仅取决于当前位置的梯度，还受到上一次参数更新方向的影响
- 在某一维度上，当梯度方向不变时，更新速度变快，当梯度方向有所改变时，更新速度变慢，从而加快收敛速度，减少震荡

## AdamGrad

对学习率进行了操作，累加梯度的平方：
$$
v_{t} = v_{t-1} + g_{t}^{2}
$$

从而自适应的改变了学习率：
$$
w_{t+1} = w_{t} - \frac{\alpha}{\sqrt{v_{t} + \epsilon}} g_{t}
$$

优点

- 自适应的改变学习率
- 能够约束梯度 适合处理稀疏梯度

缺点

- 仍依赖于人工设置一个全局学习率
- **累加之前所有的梯度平方**，中后期，分母上梯度平方的累加将会越来越大，**使得训练提前结束**

## RMSdrop

优化Adagrad，指数滑动平均累积一定历史范围的梯度平方：
$$
\begin{aligned}
& \ \ s_{t}=\beta s_{t-1} + (1-\beta)g_{t}^{2} \\
& \ \ v_{t}=g_{t}
\end{aligned}
$$
则梯度下降更新为:  
$$
w_{t+1} = w_{t} - \alpha \frac{g_{t}}{\sqrt{s_{t}}}
$$  

优点

- 适合处理非平稳目标 - 对于RNN效果很好
- 指数滑动平均累积一定范围的梯度平方

## Adam - adaptive moment estimation

把一阶动量和二阶动量结合，Adaptive + Momentum

$$
\begin{aligned}
&\ \ v_{t}=\beta_{1} v_{t-1} + (1-\beta_{1})g_{t} \\ 
&\ \ s_{t}=\beta_{2} s_{t-1} + (1-\beta_{2})g_{t}^{2} 
\end{aligned}
$$

由于 $v_{t}$，$s_{t}$ 初始化为0，且 $\beta_{1}$ , $\beta_{1}$ 均接近1，更新过程中尤其是初始阶段需要有一个偏差校正
$$
\begin{aligned}
&\ \  \hat{v_{t}} = \frac{v_{t}}{1- \beta_{1}^{t}} \\
&\ \  \hat{s_{t}} = \frac{s_{t}}{1- \beta_{2}^{t}}
\end{aligned}
$$
则梯度下降更新为:  
$$
w_{t+1} = w_{t} - \alpha \frac{\hat{v_{t}}}{\sqrt{\hat{s_{t}}} + \epsilon}
$$  

优点

- 更新的步长能够被限制在大致的范围内
- 适合解决含大规模数据和参数的优化问题
- 适用于非稳态（non-stationary）目标
- 适用于解决包含很高噪声或稀疏梯度的问题

缺点 [参考](https://zhuanlan.zhihu.com/p/32698042)

- 权重衰减和L2不等价，需要解耦学习率和权重衰减（**Adamw**）
  
- 指数移动均值本身缺陷，用**AMSGrad**
  **解释**：如某个mini-batch出现大信息量的梯度，但由于这类mini-batch出现频次很少，而指数移动均值会减弱他们的作用（**因为当前梯度权重 及当前梯度的平方的权重 ，权重都比较小**），导致在这种场景下收敛比较差

**总结**：一阶动量改变梯度，二阶动量改变学习率
