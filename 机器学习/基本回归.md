# 回归
![屏幕快照 2020-01-29 上午1.26.50](/assets/屏幕快照%202020-01-29%20上午1.26.50.png)
## 线性回归

线性回归属于回归问题，损失函数是**MSE**，可用最小二乘法和梯度下降法解决

### 1. 最大似然推导目标函数
模型为：$y^{i} = \theta^{T}x^{i} + \epsilon$，其中$\epsilon$是误差，优化目标是最小化误差，基于两个重要假设：**样本独立同分布**，**误差服从高斯分布**
- $\epsilon$ 服从高斯分布
     $$
     p(\epsilon) = \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{\epsilon^2}{2\sigma^2})
     $$
- 变换 $\epsilon = y - \theta^Tx$
     $$
     p(y|x;\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y - \theta^Tx)^2}{2\sigma^2})
     $$
     解释：参数固定时，给定一个 $x$，可以求得一个 $y$ 的概率密度


- 最大似然法
  1）样本独立同分布，联合概率密度等于边缘概率密度的乘积

    $$
    \begin{aligned}
    L(\theta) & = \prod p(y|x;\theta) \\
    & = \prod \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y - \theta^Tx)^2}{2\sigma^2})
    \end{aligned}
    $$

  2）加入$\log$，乘法变加法
  $$
  \begin{aligned}
  l(\theta) & = \log L(\theta) \\
  & = \log \prod \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y - \theta^Tx)^2}{2\sigma^2}) \\ 
  & = \sum \log \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y - \theta^Tx)^2}{2\sigma^2}) \\
  & = m\log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{2\sigma^2}\sum(y - \theta^Tx)^2
  \end{aligned}
  $$
  
  3）得到最终MSE形式loss
    $$
    J(\theta) = \frac{1}{2m}\sum_1^m(y - \theta^Tx)^2
    $$
  
### 2. 求解目标函数
&emsp;目标函数是$J(\theta) = \frac{1}{2m}\sum_1^m(y - \theta^Tx)^2$
- 最小二乘法
  $$
  \begin{aligned}
  & J(\theta) = \frac{1}{2m}(X\theta-y)^T (X\theta-y)  \\
  & \theta = (X^TX)^{-1}Xy 
  \end{aligned}
  $$
 
 > 最小二乘法要保证$X^TX$可逆，主对角线加一个$\lambda$确保正定，该方法缺点在于当维度很高时，计算量很大。

- 梯度下降法
  $$
  w = w - \alpha \frac{\partial L}{\partial w}
  $$


## Logistic回归

属于分类问题，利用sigmoid函数将输出限制到0～1，损失函数是交叉熵

### 1. 最大似然推导目标函数
二分类概率分布可以整合为一个式子，即：
$$
\begin{aligned}
& p(y=1|x;\theta)=h(x) \\
& p(y=0|x;\theta)=1-h(x) \\
& p(y|x:\theta)=h(x)^{y}(1-h(x))^{(1-y)}
\end{aligned}
$$

然后利用最小二乘法：
$$
\begin{aligned}
l(\theta) & = \log \prod p(y|x;\theta) \\
& = \log \prod h(x)^{y}(1-h(x))^{(1-y)} \\
& = \sum y\log h(x) + (1-y)\log(1-h(x))
\end{aligned}
$$

### 2. 求目标函数
梯度下降，学会交叉熵求导


## softmax回归
多分类问题，基于$\log(e^x + e^y)$等价于$\max(x, y)$：

$$
softmax = \frac{e^{x_{i}}}{\sum e^{x_{i}}}
$$


## 问题汇总
### 1. 为什么分类用交叉熵而不是MSE
- 交叉熵是凸函数，MSE是非凸函数
  - [使用凸函数可以求得全局极小值点，用欧氏距离则无法保证](https://www.zhihu.com/question/314185485)
  - 对神经网络来说，二者都不是凸优化，但用后者在收敛速度，收敛效果上更优，这也是本质问题  
- 交叉熵梯度传播友好，MSE 有梯度消失风险
- 交叉熵只针对正样本，MSE不仅针对正样本，还要平均负样本，这在分类问题中没有必要

### 2. softmaxloss和多个logistic使用环境
[Reference](../深度学习/SoftmaxLoss.md)