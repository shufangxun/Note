# 评价指标

## TPR & FPR

真正率和假正率，用来判断模型预测性能
$$
\begin{aligned}
& TPR = \frac{TP}{TP+FN} \\  
& FPR = \frac{FP}{FP+TN}
\end{aligned}
$$
  
## ROC & AUC

### 1. ROC曲线
![屏幕快照 2020-01-29 上午1.41.53](/assets/屏幕快照%202020-01-29%20上午1.41.53.png)

- ROC曲线上的每一个点对应于一个threshold，对于一个分类器，每个threshold下会有一个TPR和FPR。threshold=1，TP=FP=0，对应于原点；threshold=0，TN=FN=0，对应于右上角的点(1,1)，故ROC曲线越靠近(0,1)点，越偏离45度对角线越好
- ROC曲线对样本不平衡鲁棒，因为$FPR＝\frac{FP}{N}$，$TPR＝\frac{TP}{T}$，是基于各自正负样本的，改变样本分布ROC曲线不会变
- 基于阈值动态变化，如右图所示，两个分布只是阈值有所重叠，而不是样本有重叠

### 2. 绘制ROC曲线

![屏幕快照 2020-01-29 下午1.54.46](/assets/屏幕快照%202020-01-29%20下午1.54.46.png)

- 将预测的score降序排列，然后从高到低设置多个阈值，得到多组（FPR，TPR）
- 阈值改变只会导致点的滑动，ROC曲线不会变

### 3. AUC
AUC为ROC曲线下的面积，表示正样本得分大于负样本的概率，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类
> The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example

- AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
- 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
- AUC = 0.5，跟随机猜测一样，模型没有预测价值。
- AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

### 4. AUC计算
  1. [积分计算](https://www.zhihu.com/search?type=content&q=AUC%E8%AE%A1%E7%AE%97)，绘制ROC曲线，计算面积
  2. 统计所有$P \cdot N$个正负样本对中，正样本得分大于负样本的数目，若得分相等，计为0.5，然后除以$P \cdot N$，时间复杂度是$O(n^2)$
  3. 按照score降序排列，正样本中score最高的位置是$rank_1$，比其小的负样本有$rank_1-M$个，第二个位置是$rank_2$，比其小的负样本有$rank_2-(M-1)$个，综得到下面公式：
    $$
    AUC = \frac{\sum_{i\subseteq pos}rank_i- \frac{M(M+1)}{2}}{MN}
    $$

> [参考](https://zhwhong.cn/2017/04/14/ROC-AUC-Precision-Recall-analysis/#%E4%BA%8C%E3%80%81ROC%E6%9B%B2%E7%BA%BF)