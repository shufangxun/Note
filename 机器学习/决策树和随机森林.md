# 决策树
##  基础树
### ID3
ID3 使用的分裂标准是信息增益，它表示已知特征A使得样本集合不确定性减少的程度
$$
G(D,A) = H(D) - H(D|A)
$$
信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”
#### 算法步骤
1. 初始化；
2. 计算信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；
3. 更新数据和特征（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；
4. 重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。
#### 缺点
- ID3 没有剪枝策略，容易过拟合；
- 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；
- 只能用于处理离散分布的特征, 没有考虑缺失值

### C4.5
在$ID3$的基础上做了一些改进，最核心的改进是把信息增益换成了信息增益比，克服了$ID3$对特征数目的偏重缺点
$$
G_{ratio}(D,A) = \frac{G(D,A)}{H_A(D)}
$$
注意，信息增益比对可取值较少的特征有偏好（**分母越小，整体越大**），因此$C4.5$使用一个启发式方法：先从候选特征中找到信息增益高于平均值的特征，再从中选择增益比最高的

#### 缺点
- 剪枝策略可以再优化；
- C4.5 用的是多叉树，用二叉树效率更高；
- C4.5 只能用于分类；
C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；


### CART
$CART$是二叉树，采用基尼指数作为二分标准，计算便捷，基尼系数越小，不纯度越低，特征越好，这和信息增益（率）正好相反。

$$
Gini(D)= \sum \frac{|C_k|}{|D|}(1- \frac{|C_k|}{|D|})
$$

基尼指数反映了从**数据集中随机抽取两个样本，其类别标记不一致的概率**，可以理解为熵模型的一阶泰勒展开

#### 剪枝策略
见参考链接


#### 回归树
CART（Classification and Regression Tree，分类回归树），从名字就可以看出其不仅可以用于分类，也可以应用于回归

回归树使用误差平方和最小准则，对于任意特征A，对应的任意划分点s两边划分成的数据集$D_1$和$D_2$ ，求出使各自均方差最小，同时均方差之和最小所对应的特征和特征值划分点：
$$
\min_{a,s}[\min_{c_1} \sum_{D_1}(y-c_1)^2 + \min_{c_2} \sum_{D_2}(y-c_2)^2]
$$
回归树输出不是类别，而是最终叶子的均值或者中位数




### 问答

**1. 决策树的两个关键问题？**
- 如何选择较优的特征属性进行分裂？每一次特征属性的分裂，相当于对数据集进行再划分，也对应了一次决策树的生长。所以说，需要定义一个目标函数
- 什么时候该停止分裂？有两种自然情况需要停止分裂，一是该节点对应的所有样本记录均属于同一类别，二是该节点对应的所有样本特征属性值均相等。但是否还有其他情况呢？


**2. 介绍一下决策树**

基于特征对数据进行分类的模型，每次选取**单一特征**进行，可以看作是if-then规则的集合，主要流程分为三个：特征选择，决策树生成，决策树剪枝

**3. 决策树怎么做的？**  

- **特征选择 ** 

  ID3: 以信息增益最大化为标准，$G(D,A) = H(D) - H(D|A)$
  
  C4.5: 以信息增益比最大化为标准
  
  CART：分类树以基尼系数最小为标准，回归树以误差平方和最小为标准
  
- **决策树生成**

  将所有数据放在根节点，根据评价指标选取特征，直到没有特征可选

- **决策树剪枝**

  决策树倾向于正确分类当前数据，泛化能力因此会变弱，剪枝就是为了减轻**过拟合问题**，自底向上处理叶子结点

**4. 为什么这么操作？**

- 为什么要信息增益最大

  $H(D|A)$ 是以特征$A$分类时样本的不确定性，为了更好地分类，希望选取$A$特征之后样本的不确定性能够尽可能小，所以采用信息增益最大化

- 为什么信息增益为指标倾向于取值较多地特征

  原因同上

**5. 决策树对样本不平衡的敏感性？**
不敏感，因为决策树使用基于类变量的划分规则去创建因此可以强制地将不同类别的样本分开。

样本不平衡的思考角度：
> 1. 数据层面（过采样、欠采样）；
> 2. 算法调参层面（代价敏感学习，LR算法class_weight参数设置）；
> 3. 尝试使用对样本类别不平衡不敏感的算法（决策树、SVM）3个层面考虑。



#### 参考链接
[1. 基础树](https://zhuanlan.zhihu.com/p/85731206)