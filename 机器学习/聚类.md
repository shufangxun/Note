# 聚类算法

聚类算法属于无监督学习算法，其目的是在完全不知道样本的可能类别的情况下根据样本特征对样本进行分类。很多时候，聚类算法也可以看作数据挖掘中的一个预处理步骤

## K-means


- **前提假设**
  数据之间的相似度可以使用欧氏距离度量(如果不能使用欧氏距离度量，要先把数据转换到能用欧氏距离度量)

- **损失函数**
目标是把n个点划分到K个聚类中，使得所有点与其所属的聚类中心点之间的距离之和最小

### 算法步骤

K-Means 利用了 EM 算法进行迭代计算
1. 选择初始化的 k 个样本作为初始聚类中心$a = {a_1, a_2, a_3..a_k}$；
2. 针对每个样本$x_i$计算它到 k 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；
3. 针对每个类别，重新计算它的聚类中心$a_i = \frac{1}{|c_i|} \sum_{x \in c_i} x$；
4. 重复上面 2 3 两步操作，直到达到某个中止条件（迭代次数、最小误差变化等）。

### 复杂度

![d06ed0feed5565f9fea9bc6131b38387.png](evernotecid://5D77BDF8-572A-49B8-8038-68E72FE186D0/appyinxiangcom/22058084/ENResource/p1045)@w=500

**伪代码**
```python
获取数据 n 个 m 维的数据
随机生成 K 个 m 维的点
while(t)
    for(int i=0;i < n;i++)
        for(int j=0;j < k;j++)
            计算点 i 到类 j 的距离
    for(int i=0;i < k;i++)
        1. 找出所有属于自己这一类的所有数据点
        2. 把自己的坐标修改为这些数据点的中心点坐标
end
```

**时间复杂度**： $O(tkmn)$，其中，t 为迭代次数，k 为簇的数目，n 为样本点数，m 为样本点维度。
**空间复杂度**： $O(m(n+k))$，其中，k 为簇的数目，m 为样本点维度，n 为样本点数。

### 算法优化

#### 初始化

初始化很关键，不好好搞会陷入局部最优解

#####  K值选择

📖：[如何确定K](http://sofasofa.io/forum_main_post.php?postid=1000282)
**1. 手肘法**
   以所有样本点到它所在的聚类的中心点的距离的和作为模型的度量，记为$D_K$，计算不同 K 下 $D_K$，当$D_K$趋于平稳时，即为肘部：
   $$
   D_K = \sum_{i=1}^{K} \sum_{x \in c_i}||x - a_i ||
   $$
   手肘法是一个经验方法，而且肉眼观察也因人而异，特别是遇到模棱两可的时候。相比于直接观察法，手肘法的一个优点是，适用于高维的样本数据。有时候人们也会把手肘法用于不同的度量上，如组内方差组间方差比。
   
**2. Gap Statistic法**

要继续使用上面的$D_K$，Gap Statistic的定义为：
$$
Gap(K) = E(\log D_K) - \log D_K
$$

这里指的是$E(\log D_K)$的期望，这个数值通常通过蒙特卡洛模拟产生，我们在样本里所在的区域中按照均匀分布随机产生和原始样本数一样多的随机样本并做 Kmeans，从而得到一个$D_K$，重复多次求平均值，就得了期望，最终可以计算 Gap Statisitc。而 Gap statistic 取得最大值所对应的 K 就是最佳的 K。

##### 初始质点的选择
1. Kmeans++
![65039fcd06c157ce8a33dbb36576b528.png](evernotecid://5D77BDF8-572A-49B8-8038-68E72FE186D0/appyinxiangcom/22058084/ENResource/p1053)@w=500

    - 可以看到距离越远的点被选为质心的概率越大
    - 简单的来说，就是 K-means++ 就是选择离已选中心点最远的点。这也比较符合常理，聚类中心当然是互相离得越远越好。
 
2. Kmeans II
Kmeans++ 缺点在于难以并行化。所以 k-means II 改变取样策略，并非每次遍历只取样一个样本，而是每次遍历取样 k 个，重复该取样过程$\log(n)$次，则得到$k \log(n)$个样本点组成的集合，然后从这些点中选取 k 个。

3. 二分 Kmeans
    - 首先将数据集看成一个簇，然后进行一次 K=2 的 K-Means 聚类，将该簇一分为二
    - 计算每个簇的误差平方和，选取误差平方和较大的簇进行 K=2 的 K-Means 聚类
    - 重复上述过程，直到达到用户指定的 K 为止

### 优缺点

**优点**
1. 容易理解，聚类效果不错，虽然是局部最优，但往往局部最优就够了，算法复杂度低，收敛快

**缺点**
1. 初始化敏感，不合适的初始化会陷入局部最优解
K-means 聚类的迭代算法实际上是 EM 算法。EM 算法解决的是在**概率模型中含有无法观测的隐含变量情况下的参数估计问题**。在 K-means 中的**隐变量是每个类别所属类别**。K-means 算法迭代步骤中的`每次确认中心点以后重新进行标记`对应 EM 算法中的 E 步`求当前参数条件下的 Expectation`。而`根据标记重新求中心点`对应 EM 算法中的 M 步`求似然函数最大化时（损失函数最小时）对应的参数 `



## GMM

高斯混合模型（Gaussian Mixed Model，GMM）也是一种常见的聚类算法，与K均值算法类似，同样使用了EM算法进行迭代计算。高斯混合模型假设每个簇的数据都是符合高斯分布（又叫正态分布）的，当前数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。理论上，高斯混合模型可以拟合出任意类型的分布。

高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来的。在该假设下，每个单独的分模型都是标准高斯模型，其均值和方差是待估计的参数。此外，每个分模型都还有一个权重参数，可以理解为权重或生成数据的概率。高斯混合模型的公式为
$$p(x) = \sum_{i=1}^k \pi N (x|\mu_i, \sigma^2)$$

### 算法步骤
1. 随机初始化 $(\pi, \mu, \sigma)$
2. 根据当前的模型参数，推断出每个点属于某个分模型的后验概率
3. 根据当前的后验概率来更新 $(\pi, \mu, \sigma)$


