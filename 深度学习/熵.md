# 熵

## 信息熵
衡量随机变量的自信息量，信息熵越大，不确定性越大
$$
H(x) = - \sum p(x)\log p(x)
$$

## 相对熵
衡量两个分布的不同，又称KL散度，两个分布越接近，相对熵越小，注意它是非对称的
$$
\begin{aligned}
D(X||Y) & = \sum p(x) \log \frac{p(x)}{p(y)} \\
     & = \sum p(x) \log p(x) - \sum p(x) \log p(y)
\end{aligned}
$$

## 交叉熵

$$
\begin{aligned}
H(X,Y) & = -  \sum p(x) \log p(y) \\
    & = D(X||Y) + H(X)
\end{aligned}
$$
所以，在分类问题中，优化交叉熵来拟合模型，原因就是P固定时，交叉熵等价于相对熵，但是蒸馏问题中，用KL散度比较好，因为分布是变化的

## 条件熵
已知随机变量$X$的情况下，随机变量$Y$的的不确定性
$$
\begin{aligned}
H(Y|X) & = H(X,Y)-H(X) \\
     & = \sum p(x)H(Y|X)
\end{aligned}
$$


## 互信息
表示两个变量共享的信息量，也就是通过Y对X不确定性的降低成都，如果相互独立，互信息为0
$$
MI(X, Y) = H(X) - H(Y|X)
$$

实际上**信息增益**就是互信息

## JS散度
JS散度度量了两个概率分布的相似度，基于KL散度的变体，解决了KL散度非对称的问题

## Wasserstein距离
Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近；而JS散度在此情况下是常量，KL散度可能无意义


## 问答
1. 为什么用交叉熵不用KL散度计算loss？
[交叉熵为什么可以用于计算Loss？](https://www.zhihu.com/question/65288314/answer/244557337)

## 参考
[2. 交叉熵、相对熵（KL散度）、JS散度和Wasserstein距离](https://zhuanlan.zhihu.com/p/74075915)