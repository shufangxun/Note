# L1 和 L2 正则化

Loss后面加上**权重的正则项**，平衡经验风险最小化和结构风险最小化，尽可能采用简单的模型，降低模型复杂度，以此提高模型泛化能力

## 理解

- 正则化产生了稀疏性（Sparsity），减少了特征向量个数，自动选择主要特征，降低了模型的复杂度，提高了泛化能力，符合**奥卡姆剃刀原理**
- 正则化项的引入利用了先验知识，$\lambda$ 的选择体现了对最优解 $w$ 的可信程度，如果 $\lambda$ 很小，那么正则化不起作用，说明所求解有很大不确定性，

**L1正则化**  
在Loss后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）

**L2正则化**  
在Loss后边所加正则项为L2范数，加上L2范数得到的解比较平滑，大部分都很小

**L1和L2正则化使用范围**  
只用于卷积层和全连接层，因为这里参数很多，如果用于BN层，参数过少，会丧失功能

## L2正则化和权重衰减的区别

- L2正则化是在目标函数中直接加上一个正则项，直接修改了优化目标；权重衰减是在训练的每一步结束的时候，对网络中的参数值**裁剪一定的比例**，优化目标是不变的
- 在梯度下降中，L2正则化等价于权重衰减
