# 最小二乘法 牛顿法 梯度下降法

## 最小二乘法

⚠️：二乘是平方的意思

最小化误差的平方和来拟合函数，适用线性回归问题，求解方法是对Loss做矩阵求导：
$$
\theta = (X^TX)^{-1}XX^TY
$$

### 优缺点
**优点**：无需迭代和调参，一步到位；
**缺点**：
- 当$X^TX$的逆矩阵不存在或者不是线性回归问题时，无法使用最小二乘法
- 当特征维度很高时，逆矩阵的计算复杂度很高

解决方案：可以通过增加正则项的方法，让$X^TX$变成满秩矩阵。

## 梯度下降法
利用一阶导信息，从数学上的角度来看，**梯度方向是函数增长速度最快的方向，那么梯度的反方向就是函数减少最快的方向**，那么最小化Loss就是负梯度方向操作，也就是梯度下降法。

### 随机梯度下降 
随机梯度下降法不同于批量梯度下降，随机梯度下降是在每次迭代时使用一个样本来对参数进行更新（mini-batch size =1）

**优点**：在学习过程中加入了噪声，提高了泛化误差；由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。
**缺点**：不收敛，在最小值附近波动；单个样本并不能代表全体样本的趋势，当遇到局部极小值或鞍点时，SGD会卡在梯度为0处。

## 牛顿法
📖：[牛顿法推导](https://zhuanlan.zhihu.com/p/59873169)

牛顿法利用的是损失函数的二阶导数信息，求解一阶导数为0时的参数，进而求出参数，收敛速度很快，$J(W)$的二阶泰勒展开为：
$$
J(W) = J(W_t) + g^T(W-W_t) + \frac{1}{2}(W-W_t)^TH(W-W_t)
$$
求极值点可得：
$$
W = W_0 -H^{-1}g
$$

所以牛顿法主要与**海森矩阵**和**一阶导数**有关，牛顿法优点是收敛速度快，特别的对于一个正定的二次函数，牛顿法只需一步即可找到最小值。

但缺点也很多：
1. 牛顿方向由Hessian矩阵的逆和梯度g确定，所以如果梯度g为0，则权重W就无法更新，梯度为0的点是极值点，不止是局部最小值、全局最小值，还包含鞍点、局部最大值、全局最大值，所以牛顿法容易陷于**极值点**；

2. 牛顿法**最致命的缺点是每一步迭代需要求解Hessian逆矩阵**，计算量大，甚至有时Hessian矩阵不可逆，从而导致这种方法失效； 

3. 牛顿法方向指向最近的极值点，哪怕这个极值点是鞍点或者局部最大值，而梯度下降一般指向$J(W)$下降最快的方向；

4. 梯度下降一般比较容易收敛到一个较小的最优解，而牛顿法在每次迭代时序列可能不会收敛到一个最优解，它甚至不能保证函数值会按照这个序列递减（这由第二点牛顿方向可知）。

解决方案：

1. 采用拟牛顿法，通过近似计算每一步的Hessian矩阵逆矩阵，从而减少逆矩阵的直接求解，减少计算量。

2. 首先深度学习的参数数量动则十万百万，非常高维，存在所有维度的局部极值点的可能性非常小，总有一部分方向指向山脚方向，从而可以助其逃脱出其他维度的局部极值；其次，深度学习训练一般是一个mini batch一个mini batch的训练，输入是变化的，即使对于上一个batch输入产生的$J(W)$陷于局部极值点，下一个batch的输入产生的$J(W)$也不一定陷于局部极值点，因为不同输入产生的$J(W)$是不同的，这也有助于局部极值点的逃脱。


## 问答

1. 随机梯度下降法的batch size可以无限大么？
   不可以，首先内存不够放；其次batchsize过大会降低泛化性能，有过拟合风险