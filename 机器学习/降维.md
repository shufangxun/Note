## 降维

## 1. PCA
无监督降维算法，提取**主要特征表示所有特征**，有最大投影方差和最小投影距离两种解释，这里主要讨论最大投影方差法

### 理解
降维实际上就是寻找一组新的基表示数据，如果基的数量少于向量本身的维数，则可以达到降维的效果。

如何选择基才是最优的，即我们应该如何选择 K 个基才能最大程度保留原有的信息？

一种直观的看法是：希望投影后**尽可能分散**，因为如果重叠就会有样本消失。当然这个也可以从熵的角度进行理解，熵越大所含信息越多。


### 实现

#### 方差和协方差

在一维情况下，数据的分散程度用方差表示：
$$
var(a) = \frac{1}{m} \sum_{i}^{m}(a_i - \mu)^2
$$
在多维情况下，用协方差进行约束，协方差表示两个变量的相关性。为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息：
$$
conv(a,b) = \frac{1}{m-1} \sum_{i}^{m}(a_i - \mu_a)(b_i - \mu_b)
$$

**当协方差为 0 时，表示两个变量不相关**，为了让协方差为 0，**第二个基必须和第一个基正交**。

至此，我们得到了降维问题的优化目标：将一组 N 维向量降为 K 维，其目标是
- **选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0**
- **在正交的约束下，变量方差则尽可能大，取最大的 K 个方差**

#### 统一到协方差矩阵

⚠️：为了计算的方便，要先中心化，也就是均值为0

我们看到，优化目标与**变量内方差**及**变量间协方差**有密切关系。因此我们希望能将两者统一表示，而两者均可以表示为内积的形式，内积又与矩阵相乘密切相关，于是：
$$
X = \begin{pmatrix}
 a_1 & a_2 &a_3\\ 
 b_1 & b_2 &b_3
\end{pmatrix}
$$

那么协方差矩阵对角线是方差，其余是协方差矩阵：
$$
\frac{1}{m}XX^T = \begin{pmatrix}
 var(a) & cov(a,b)\\ 
 cov(b,a) &  var(b)
\end{pmatrix}
$$
因此优化目标是：**将除对角线外的其它元素化为 0，并且在对角线上将元素按大小从上到下排列（变量方差尽可能大）**

#### 矩阵对角化

设原始数据矩阵 X 对应的协方差矩阵为 C，而 P 是一组基按行组成的矩阵，设 Y=PX，则 Y 为 X 对 P 做基变换后的数据。设 Y 的协方差矩阵为 D，推导一下 D 与 C 的关系：
$$
\begin{aligned}
D & = \frac{1}{m} YY^T \\
  & = \frac{1}{m} (PX)(PX)^T \\
  & = \frac{1}{m} PXX^TP^T \\
  & = P (\frac{1}{m} XX^T)P^T \\
  & = PCP^T
\end{aligned}
$$

所以优化目标：寻找一个矩阵 P，满足$D = PCP^T$是一个对角矩阵，并且对角元素按从大到小依次排列，那么 P 的前 K 行就是要寻找的基，用 P 的前 K 行组成的矩阵乘以 X 就使得 X 从 N 维降到了 K 维并满足上述优化条件。

至此，我们离 PCA 还有仅一步之遥，我们还需要完成对角化。
由上文知道，协方差矩阵 C 是一个是对称矩阵，在线性代数中实对称矩阵有非常好的性质：**实对称矩阵不同特征值对应的特征向量必然正交**，所以求原始数据的协方差矩阵的特征值和特征向量就行。

#### 求解步骤
设有 m 条 n 维数据。

- 将 X 的**每一行进行零均值化**，即减去这一行的均值；
- 求出协方差矩阵的特征值及对应的特征向量；
- 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P；
- Y = PX 即为降维到 k 维后的数据。

### 


## 2. LDA

有监督降维，基于类间方差最大和类内方差最小，解决PCA的类别划分不清楚问题


## 3. 与SVD的区别
PCA 需要对协方差矩阵$\frac{1}{m}XX^T$进行特征值分解； SVD 也是对$AA^T$进行特征值分解，当：
$$
A = \frac{X^T}{\sqrt{m}}
$$

则**两者基本等价**，所以 PCA 问题可以转换成 SVD 求解

实际上 Sklearn 的 PCA 就是用 SVD 进行求解的，原因有以下几点：
1. 当样本维度很高时，协方差矩阵计算太慢；
2. 方阵特征值分解计算效率不高；SVD 除了特征值分解这种求解方式外，还有更高效迭代求解方式；
3. 其实 PCA 与 SVD 的右奇异向量的压缩效果相同。
## 4. 问答

- **为什么需要降维**

  数据在高维冗余，如三维空间上的一个平面，在合适的坐标变化下可以在二维空间上清晰表示，所以需要降到低维。

- **降维和特征提取的区别**

  降维是在低维生成新的有代表性的特征；特征提取是按照重要性选择特征

- **PCA可以用于降低过拟合么**
  不可以，因为方差小的非主成分也可能含有对样本差异的重要信息，降维丢弃可能对后续数据处理有影响，所以不适合用于数据避免过拟合

- **PCA为什么先均值化**
  为了方便方差计算
- **PCA和LDA区别**  

  PCA是无监督降维，不会考虑类别，而LDA是考虑类别的，比如在语言识别时，先用PCA去掉固定的噪声，然后用LDA，区分每一个人的声音